---
title: "RMSC5001 Project 2018-2019 - Classification Tree"
author: CHING, Pui Chi 1155102106 <br/> MA, Cheuk Fung 1155106595 <br/> YEUNG, Ka Ming 1155104060
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
    toc_depth: '5'
  html_document:
    df_print: paged
    highlight: pygments
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 5
    toc_float: yes
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 150)
```

# Introduction
>“The world cannot be understood without numbers. But the world cannot be understood with numbers alone.”
>― *Hans Rosling, Factfulness: Ten Reasons We're Wrong About the World—and Why Things Are Better Than You Think*[^1]

[^1]:  Factfulness: Ten Reasons We're Wrong About the World – and Why Things Are Better Than You Think by Hans Rosling, Ola Rosling, Anna Rosling Rönnlund. ISBN 9781473637467

When we were young, teachers often taught us the differences between developed and developing countries. The simplest way to define a country whether it is developed or developing is by GDP per capita. Since 2016, the World Bank is no longer distinguishing the term developed and developing. Instead, the World Bank classify countries by income: “low-income, lower-middle-income, upper-middle income and high-income economies”. Now we are interested to know which factors contribute to levels of income. We would like to have decision rules and a model to help us to determine the income group of countries.

# Set Up
## Library
```{r load packages, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(rworldmap)
library(knitr)

library(rpart)
library(rpart.plot)
library(rattle)
library(party)
library(tidyverse)
```

## Dataset
We extracted data which cover dependent variable of region, GDP, labor force, life expectancy, military expense, high technology export and corruption for total 177 countries. Some data are missing as military expense, high technology export data and corruption are unknown for some countries in the World Bank database. We have modified them as zero to setup usable dataset for our case study. We have used income group as the target variable. There are 4 different income groups as prediction results. All the data are retrieved from the World Bank database[^2].

[^2]: https://databank.worldbank.org/data/home.aspx

## Data field legend
```{r data field, echo=FALSE, results='asis'}
income <- read.csv("worldbank_income.csv")
kable(income, caption = "World Bank definition on the income group")

field <- read.csv("worldbank_field.csv")
kable(field, caption = "Field information")
```

## Data exploration
```{r read data}
d <- read.csv("world.csv", header = TRUE)
n <- nrow(d) # get sample size
set.seed(12345) # set random seed
r <- 0.80 # set sampling ratio
id <- sample(1:n, size = round(r * n)) # generate id
train <- d[id, ] # training dataset
test <- d[-id, ]
head(d)
```

## Different income levels around the World
```{r world map}
income.df <- joinCountryData2Map(d, 
                                 joinCode = "NAME", 
                                 nameJoinColumn = "COUNTRY")

mapParams <- mapCountryData(income.df, 
               nameColumnToPlot = "INCOME_GROUP", 
               catMethod = "categorical", 
               colourPalette = "diverging", 
               addLegend='FALSE',
               mapTitle = "Different Income Levels around the World")

do.call(addMapLegendBoxes, 
        c(mapParams, 
          x = 'bottomleft', 
          horiz = FALSE, 
          title = "", 
          bg = "NA", 
          col = "NA"))
```

## Different income groups probability
```{r plot income group number}
# All data
prop.table(table(d$INCOME_GROUP))
all_plot <- d[, -1] %>%
  count(INCOME_GROUP) %>%
  ggplot() +
  geom_col(aes(x = INCOME_GROUP, y = n),
    fill = "#002244",
    colour = "#002244"
  ) +
  ggtitle("Income group (All data)") +
  coord_flip() +
  theme_fivethirtyeight()

# Training data
prop.table(table(train$INCOME_GROUP))
train_plot <- train[, -1] %>%
  count(INCOME_GROUP) %>%
  ggplot() +
  geom_col(aes(x = INCOME_GROUP, y = n),
    fill = "#002244",
    colour = "#002244"
  ) +
  ggtitle("Income group (Training data)") +
  coord_flip() +
  theme_fivethirtyeight()

# Testing data
prop.table(table(test$INCOME_GROUP))
test_plot <- test[, -1] %>%
  count(INCOME_GROUP) %>%
  ggplot() +
  geom_col(aes(x = INCOME_GROUP, y = n),
    fill = "#002244",
    colour = "#002244"
  ) +
  ggtitle("Income group (Testing data)") +
  coord_flip() +
  theme_fivethirtyeight()

grid.arrange(all_plot, train_plot, test_plot, nrow = 3, ncol = 1)
```

# Methodology
Classification is a predictive model .The main use is to utilize the data we have and identity the relevant factors hence classify the out-sample data into correct group and classes. 
The classification tree was built based on the binary splitting of variables, one at a time. The tree is constructed such that each terminal node is as “pure” as possible. That is, in each terminal node, most of the observations are belongs to the same group. Once the classification tree is built, a set of simple classification rules can be easily obtained. Since this method will search for all possible binary splitting of the variables, it is computational intensive and may be time consuming.

## Classification Tree
```{r}
ctree <- rpart(INCOME_GROUP ~
REGION + GDP +
  LABOUR_FORCE +
  LIFE_EXPECTANCY +
  MILITARY_EXPENSE +
  HIGH_TECHNOLOGY_EXPORT +
  CORRUPTION,
data = train,
method = "class",
control = rpart.control(cp = 0)
)
```

## Plot Classification Tree
### Classification Tree in Percentage
```{r}
rpart.plot(ctree, extra = 104)
```

### Classification Tree in Numbers
```{r}
rpart.plot(ctree, extra = 101)
```

## Print classification Tree
```{r}
print(ctree)
```

### Classification tree discussion
Among of 7 variables we input into the model, only 4 were selected by the classification tree. It is not hard to see life expectancy is the key variable that to determine whether a country is high or low-income economy. The first two terminal nodes (with high-income and low-income label) are very straight forward and easy to identify. Due to these two group are the extreme one, it is easy to explain with selected variable(s). 30% of the population (data) directly go to the 1st terminal node with label of high-income group. It is determined by just 1 variable - life expectancy >=77. Although there are some misclassifications for upper middle-income economies to the high-income economies, think logically, people in a higher-income economy always have longer life due to better standard of living. The 2nd terminal node with label of low-income account for 15% of population. It is also clear cut that when all the negative factors appeared at once (corruption is high, life expectancy is short, and technology is poor), it goes straight to low-income economy. Again, there are some misclassification for lower middle-income economies to low-income economies. Both terminal nodes have a very high confidence (low error rate). The rest of the population (~55%) were distributed in 6 terminal nodes with the label of lower/upper-middle income. The error rate is quite high (>20%). It may due to the reason that the selected factors are not significant to define the income group in the middle range.

## Decision Rule:
```{r}
rpart.rules(ctree)
```

From above result, we can conclude total 8 significant classification paths as below :

R1: If Life Expectancy > 77 then HIGH INCOME (35/0/0/7)

R2: If Life expectancy < 66 and Corruption level >0.75 and Export is less than 9.45 million then LOW INCOME (0/18/4/0)

R3: If Life expectancy < 66 and Corruption level >0.75 and Export is more than 9.45million then LOW MIDDLE INCOME (0/3/4/0)

R4: If Life expectancy is < 74  , Corruption level >0.75 and export is more than 18 million then LOW MIDDLE INCOME (0/0/7/3)

R5: If Life Expectancy is in between 66 vs 77 and Corruption level >0. 75 and  Labor force is more than 356,510 then LOW MIDDLE INCOME (0/4/12/0)

R6: If Life Expectancy is in between 66 vs 77 and Corruption level >0. 75 and  Labor force is less than 356,510 then UPPER MIDDLE INCOME (0/0/3/4)

R7: If Life expectancy is between 74 vs 77  , Corruption level >0.75 then UPPER MIDDLE INCOME (5/0/3/17)

R8: If Life Expectancy is < 74 and Corruption level < 0. 75 and export level is less than 18 million then UPPER MIDDLE INCOME (0/0/3/10)

## Support, Confident and Capture of the Classification Tree
```{r rule results, echo=FALSE, results='asis'}
rule.results <- read.csv("rule_results.csv")
kable(rule.results, caption = "Classification Tree Explanatory Power")
```

Rule 1 and 2 have better predictive power with more than 80% confident level.

## Investigate the complexity plot
```{r tree complexity}
printcp(ctree)
plotcp(ctree)
```

# Prediction
## Prediction in training dataset
```{r ,echo=FALSE, eval=FALSE}
# procedure in lecture notes
predict_train <- predict(ctree)
cl_train <- max.col(predict_train)
table(cl_train, train$INCOME_GROUP)
```

```{r training prediction}
predict_train2 <- predict(ctree, train, type = "class")
table(predict_train2, train$INCOME_GROUP)
```

## Prediction in testing dataset
```{r , echo=FALSE, eval=FALSE}
# procedure in lecture notes
predict_test <- predict(ctree, test) # out-sample
cl_test <- max.col(predict_test)
table(cl_test, test$INCOME_GROUP)
```

```{r testing prediction}
predict_test2 <- predict(ctree, test, type = "class")
table(predict_test2, test$INCOME_GROUP)
```

## Accuracy
```{r}
base_accuracy <- mean(predict_test2 == test$INCOME_GROUP)
(base_accuracy)
```

## Improve classification tree by pruning

### Prepruning
```{r}
# Grow a tree with minsplit of 100 and max depth of 8
ctree_preprun <- rpart(INCOME_GROUP ~
REGION + GDP +
  LABOUR_FORCE +
  LIFE_EXPECTANCY +
  MILITARY_EXPENSE +
  HIGH_TECHNOLOGY_EXPORT +
  CORRUPTION,
data = train, method = "class",
control = rpart.control(cp = 0, maxdepth = 8, minsplit = 100)
)
# Compute the accuracy of the pruned tree
test$pred <- predict(ctree_preprun, test, type = "class")
accuracy_preprun <- mean(test$pred == test$INCOME_GROUP)
```

### Postpruning
```{r}
# Prune the ctree_model based on the optimal cp value
ctree_pruned <- prune(ctree, cp = 0.0046)

# Compute the accuracy of the pruned tree
test$pred <- predict(ctree_pruned, test, type = "class")
accuracy_postprun <- mean(test$pred == test$INCOME_GROUP)
data.frame(base_accuracy, accuracy_preprun, accuracy_postprun)
```

